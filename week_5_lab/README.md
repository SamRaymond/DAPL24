# Week 5 Lab  - Retrieval-Augmented Generation (RAG) with LlamaIndex

### Motivation
In this class we will look at retrieval augmented generation (RAG), one of the most common and high demanded use cases of LLMs in enterprise. The goal of RAG is to avoid having to finetune and model, and to reduce the occurance of halluncinations. In RAG we have to build a few pieces, including a vector database so we can search documents based on the input query. In this lab you will be build a RAG pipeline using a bespoke library, `LlamaIndex`.

### Learning Objectives
RAG requires a few different pieces, including managing data and converting them into a "chunked" database that can then be onverted to vectors using an embedding LLM so they can be searched against a user query and then used to generate the response. By the end of this lab you will:
- Create a vector database using a series of documents.
- Connect GPT-4 with your documents. 
- Use `LlamaIndex` to build and end-to-end RAG pipeline.
- Deploy a RAG pipeline with Gradio


## Part 1 - Getting Familiar with LlamaIndex and the RAG paradigm

  
## Part 2 - Building a RAG workflow

  
## Part 3 - Creating a RAG app with Gradio
